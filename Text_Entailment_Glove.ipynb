{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Entailment Glove",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TLCCdl1dLdB",
        "colab_type": "text"
      },
      "source": [
        "I have uploaded the train.jsonl, test.jsonl and dev.jsonl on my google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA0SsVlkLB0e",
        "colab_type": "code",
        "outputId": "6d4915f3-3162-4f88-e401-fdbc85882058",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "!pip install json-lines"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting json-lines\n",
            "  Downloading https://files.pythonhosted.org/packages/7f/0f/79c96c0d26b276c583484fe8209e5ebbb416a920309568650325f6e1de73/json_lines-0.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from json-lines) (1.11.0)\n",
            "Installing collected packages: json-lines\n",
            "Successfully installed json-lines-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfXL5lmTeF42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json_lines\n",
        "import json\n",
        "import pandas as pd\n",
        "import requests\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from keras.utils import plot_model\n",
        "import keras.layers\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras import optimizers\n",
        "from sklearn.preprocessing import OneHotEncoder\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w73hxUbGeRhK",
        "colab_type": "code",
        "outputId": "5b7b9bef-9812-4062-8ded-1583a1ef7385",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7874
        }
      },
      "source": [
        "#getting dataset from git repo. I uploaded the files in the Text-Entailment repository\n",
        "\n",
        "req1 = requests.get('https://raw.githubusercontent.com/tanishkasingh9/Text-Entailment/master/dev.jsonl')\n",
        "c1 = req1.content\n",
        "req2 = requests.get('https://raw.githubusercontent.com/tanishkasingh9/Text-Entailment/master/train.jsonl')\n",
        "c2 = req2.content\n",
        "req3 = requests.get('https://raw.githubusercontent.com/tanishkasingh9/Text-Entailment/master/test.jsonl')\n",
        "c3 = req3.content\n",
        "\n",
        "#print(c)\n",
        "\n",
        "dev = pd.read_json( c1, lines=True)\n",
        "train = pd.read_json( c2, lines=True)\n",
        "test = pd.read_json( c3, lines=True)\n",
        "print(dev)\n",
        "\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    annotator_labels       genre     gold_label   pairID  promptID  \\\n",
            "0       [entailment]      travel     entailment    8608e      8608   \n",
            "1    [contradiction]       slate  contradiction   51889c     51889   \n",
            "2          [neutral]   telephone        neutral   72610n     72610   \n",
            "3    [contradiction]      travel  contradiction   58952c     58952   \n",
            "4    [contradiction]     fiction  contradiction   61024c     61024   \n",
            "5          [neutral]   telephone        neutral   60546n     60546   \n",
            "6          [neutral]   telephone        neutral   27530n     27530   \n",
            "7       [entailment]   telephone     entailment   30033e     30033   \n",
            "8    [contradiction]     fiction  contradiction   35087c     35087   \n",
            "9       [entailment]     fiction     entailment   62291e     62291   \n",
            "10   [contradiction]     fiction  contradiction   23916c     23916   \n",
            "11         [neutral]       slate        neutral    9967n      9967   \n",
            "12         [neutral]       slate        neutral  115147n    115147   \n",
            "13      [entailment]     fiction     entailment   31651e     31651   \n",
            "14      [entailment]       slate     entailment    7139e      7139   \n",
            "15   [contradiction]   telephone  contradiction  124385c    124385   \n",
            "16      [entailment]     fiction     entailment   11758e     11758   \n",
            "17      [entailment]     fiction     entailment   51208e     51208   \n",
            "18         [neutral]     fiction        neutral   64178n     64178   \n",
            "19   [contradiction]     fiction  contradiction   60602c     60602   \n",
            "20         [neutral]   telephone        neutral  104379n    104379   \n",
            "21      [entailment]   telephone     entailment   94388e     94388   \n",
            "22         [neutral]   telephone        neutral  124910n    124910   \n",
            "23         [neutral]   telephone        neutral   84733n     84733   \n",
            "24   [contradiction]     fiction  contradiction   60941c     60941   \n",
            "25   [contradiction]  government  contradiction  125103c    125103   \n",
            "26      [entailment]  government     entailment    5170e      5170   \n",
            "27      [entailment]   telephone     entailment  122150e    122150   \n",
            "28   [contradiction]      travel  contradiction   19324c     19324   \n",
            "29         [neutral]      travel        neutral    3082n      3082   \n",
            "..               ...         ...            ...      ...       ...   \n",
            "970        [neutral]     fiction        neutral   82604n     82604   \n",
            "971        [neutral]       slate        neutral   98958n     98958   \n",
            "972     [entailment]     fiction     entailment   31725e     31725   \n",
            "973  [contradiction]     fiction  contradiction   46552c     46552   \n",
            "974  [contradiction]       slate  contradiction   43052c     43052   \n",
            "975     [entailment]  government     entailment  145673e    145673   \n",
            "976     [entailment]  government     entailment   18431e     18431   \n",
            "977     [entailment]  government     entailment  139372e    139372   \n",
            "978     [entailment]     fiction     entailment   63002e     63002   \n",
            "979        [neutral]     fiction        neutral  146021n    146021   \n",
            "980  [contradiction]     fiction  contradiction    2429c      2429   \n",
            "981        [neutral]   telephone        neutral   77470n     77470   \n",
            "982        [neutral]  government        neutral  115039n    115039   \n",
            "983     [entailment]  government     entailment  140195e    140195   \n",
            "984        [neutral]   telephone        neutral   39311n     39311   \n",
            "985     [entailment]      travel     entailment   81976e     81976   \n",
            "986        [neutral]  government        neutral   47100n     47100   \n",
            "987        [neutral]   telephone        neutral   23432n     23432   \n",
            "988  [contradiction]       slate  contradiction   24232c     24232   \n",
            "989  [contradiction]     fiction  contradiction  105249c    105249   \n",
            "990        [neutral]   telephone        neutral   93390n     93390   \n",
            "991     [entailment]     fiction     entailment     804e       804   \n",
            "992     [entailment]   telephone     entailment   63971e     63971   \n",
            "993  [contradiction]     fiction  contradiction   88140c     88140   \n",
            "994     [entailment]       slate     entailment   36713e     36713   \n",
            "995  [contradiction]     fiction  contradiction   13272c     13272   \n",
            "996        [neutral]  government        neutral   95288n     95288   \n",
            "997     [entailment]      travel     entailment    9477e      9477   \n",
            "998        [neutral]     fiction        neutral   63771n     63771   \n",
            "999        [neutral]   telephone        neutral   29816n     29816   \n",
            "\n",
            "                                             sentence1  \\\n",
            "0    The guardians are particularly helpful and att...   \n",
            "1                      She might also consider moving.   \n",
            "2         oh it's not it's probably not quite the same   \n",
            "3    Rooms are all different, but singles are disap...   \n",
            "4                            Mr. Brown took his place.   \n",
            "5                       in cold frames or whatever the   \n",
            "6                           they had quite a good time   \n",
            "7                                 long haired yeah and   \n",
            "8    He's got to do it himself,' Lincoln said fierc...   \n",
            "9                           Well, then, let's go back.   \n",
            "10    Mother tells me you were out early this morning.   \n",
            "11                               Renaissance Florence.   \n",
            "12   Nebbishes are too pathetic to warrant actual d...   \n",
            "13      She was frail and couldn't move her left side.   \n",
            "14                          Operators are standing by.   \n",
            "15           he hit her in the stomach with the shovel   \n",
            "16                    We will live like rats in holes.   \n",
            "17   I saw two armoured officers, ripe for the pluc...   \n",
            "18               Why, yes, came the answering thought.   \n",
            "19   Or, rather, pursued my friend imperturbably, \"...   \n",
            "20           that yeah i guess that takes effect in uh   \n",
            "21             well Rick it's been good talking to you   \n",
            "22                  and almost everyone here has cable   \n",
            "23   all right Ron we'll see you later all right by...   \n",
            "24            The audience wanted to see her struggle.   \n",
            "25                                individual offenders   \n",
            "26       Use a 360-degree performance feedback system.   \n",
            "27   hum so you're fishing downstream so it will ca...   \n",
            "28           Rooms are classically decorated and warm.   \n",
            "29             The church dates from the 14th century.   \n",
            "..                                                 ...   \n",
            "970                         He pointed out the crooks.   \n",
            "971               That's Bradley's real casting couch.   \n",
            "972  It was beautifully done soberly and with perfe...   \n",
            "973                                 All right say it!    \n",
            "974       John maintains his innocence from death row.   \n",
            "975                        It's easy, said Adams, 25.    \n",
            "976                  The OTC consists of the Governor    \n",
            "977                            Ranking Minority Member   \n",
            "978                                This was different.   \n",
            "979                           It is at that, said Jon.   \n",
            "980       Brill grazed in huge fields of golden wheat.   \n",
            "981         uh-huh sounds like it  do you like to cook   \n",
            "982  Brief interventions for patients with alcohol-...   \n",
            "983        Implementing screening in clinical practice   \n",
            "984                             no no i live in Dallas   \n",
            "985                               Dante writes Inferno   \n",
            "986                Industrial Economics, Incorporated.   \n",
            "987     um-hum absolutely you need about even probably   \n",
            "988                               Calling Frank Capra!   \n",
            "989            A free ebook from http://manybooks.net/   \n",
            "990                    yeah your defeating the purpose   \n",
            "991              I can only know how I feel right now.   \n",
            "992                              really are you a yeah   \n",
            "993                            Let's go and get them!\"   \n",
            "994                        as if we were by ourselves,   \n",
            "995                      The northerner breathed hard.   \n",
            "996                      Over the entrance to the U.S.   \n",
            "997       Ponce de Leen also suffered another setback.   \n",
            "998                   Ca'daan realized it was a smile.   \n",
            "999                   oh yeah i caught the end of that   \n",
            "\n",
            "                                sentence1_binary_parse  \\\n",
            "0    ( ( The guardians ) ( ( are ( particularly ( (...   \n",
            "1    ( She ( ( ( might also ) ( consider moving ) )...   \n",
            "2    ( ( oh ( it ( 's ( not ( it ( ( 's probably ) ...   \n",
            "3    ( ( ( ( ( Rooms ( ( are all ) different ) ) , ...   \n",
            "4       ( ( Mr. Brown ) ( ( took ( his place ) ) . ) )   \n",
            "5    ( ( ( in ( cold frames ) ) or ) ( whatever the...   \n",
            "6       ( they ( had ( quite ( a ( good time ) ) ) ) )   \n",
            "7                     ( long ( haired ( yeah and ) ) )   \n",
            "8    ( ( He ( 's ( got ( to ( ( do it ) himself ) )...   \n",
            "9    ( Well ( , ( then ( , ( ( let 's ) ( ( go back...   \n",
            "10   ( ( Mother ( tells me ) ) ( you ( ( ( were out...   \n",
            "11                      ( Renaissance ( Florence . ) )   \n",
            "12   ( Nebbishes ( ( are ( too ( pathetic ( to ( wa...   \n",
            "13   ( She ( ( ( ( was frail ) and ) ( ( could n't ...   \n",
            "14         ( Operators ( ( are ( standing by ) ) . ) )   \n",
            "15   ( he ( ( ( hit her ) ( in ( the stomach ) ) ) ...   \n",
            "16   ( We ( ( will ( live ( like ( rats ( in holes ...   \n",
            "17   ( I ( ( saw ( ( ( two ( armoured officers ) ) ...   \n",
            "18   ( ( ( Why , ) yes ) ( , ( ( came ( the ( answe...   \n",
            "19   ( Or ( , ( rather ( , ( ( ( ( ( ( pursued ( my...   \n",
            "20   ( ( that ( yeah i ) ) ( guess ( that ( ( takes...   \n",
            "21   ( well ( Rick ( it ( 's ( been ( good ( talkin...   \n",
            "22   ( and ( ( almost ( everyone here ) ) ( has cab...   \n",
            "23   ( ( all right ) ( Ron ( we ( 'll ( see ( you (...   \n",
            "24   ( ( The audience ) ( ( wanted ( to ( see ( her...   \n",
            "25                            ( individual offenders )   \n",
            "26   ( ( Use ( a ( 360-degree ( performance ( feedb...   \n",
            "27   ( hum ( so ( you ( 're ( fishing ( downstream ...   \n",
            "28   ( Rooms ( ( are ( ( ( classically decorated ) ...   \n",
            "29   ( ( The church ) ( ( dates ( from ( the ( 14th...   \n",
            "..                                                 ...   \n",
            "970    ( He ( ( ( pointed out ) ( the crooks ) ) . ) )   \n",
            "971  ( That ( ( 's ( ( Bradley 's ) ( real ( castin...   \n",
            "972  ( It ( ( was ( beautifully ( done ( ( soberly ...   \n",
            "973                 ( ( All right ) ( ( say it ) ! ) )   \n",
            "974  ( John ( ( ( maintains ( his innocence ) ) ( f...   \n",
            "975  ( ( It ( 's easy ) ) ( , ( ( said ( ( Adams , ...   \n",
            "976  ( ( The OTC ) ( consists ( of ( the Governor )...   \n",
            "977                    ( Ranking ( Minority Member ) )   \n",
            "978                   ( This ( ( was different ) . ) )   \n",
            "979  ( ( It ( is ( at that ) ) ) ( , ( ( said Jon )...   \n",
            "980  ( Brill ( ( grazed ( in ( ( huge fields ) ( of...   \n",
            "981  ( uh-huh ( sounds ( like ( it ( do ( you ( lik...   \n",
            "982  ( Brief ( ( interventions ( for ( patients ( w...   \n",
            "983  ( ( Implementing screening ) ( in ( clinical p...   \n",
            "984         ( ( no ( no i ) ) ( live ( in Dallas ) ) )   \n",
            "985                       ( Dante ( writes Inferno ) )   \n",
            "986  ( ( ( ( Industrial Economics ) , ) Incorporate...   \n",
            "987  ( um-hum ( absolutely ( you ( ( need ( about e...   \n",
            "988                  ( Calling ( Frank ( Capra ! ) ) )   \n",
            "989  ( ( A ( free ebook ) ) ( from http://manybooks...   \n",
            "990    ( ( yeah ( your defeating ) ) ( the purpose ) )   \n",
            "991  ( I ( ( ( can only ) ( know ( how ( I ( feel (...   \n",
            "992              ( really ( ( are you ) ( a yeah ) ) )   \n",
            "993  ( ( Let 's ) ( ( ( ( ( go and ) get ) them ) !...   \n",
            "994  ( as ( if ( we ( ( were ( by ourselves ) ) , )...   \n",
            "995     ( ( The northerner ) ( ( breathed hard ) . ) )   \n",
            "996  ( Over ( ( the entrance ) ( to ( the U.S. ) ) ) )   \n",
            "997  ( ( Ponce ( de Leen ) ) ( also ( ( suffered ( ...   \n",
            "998  ( ( Ca ( ` daan ) ) ( ( realized ( it ( was ( ...   \n",
            "999  ( ( oh ( yeah i ) ) ( caught ( ( the end ) ( o...   \n",
            "\n",
            "                                       sentence1_parse  \\\n",
            "0    (ROOT (S (NP (DT The) (NNS guardians)) (VP (VB...   \n",
            "1    (ROOT (S (NP (PRP She)) (VP (MD might) (ADVP (...   \n",
            "2    (ROOT (SINV (VP (VBD oh) (SBAR (S (NP (PRP it)...   \n",
            "3    (ROOT (S (S (NP (NNS Rooms)) (VP (VBP are) (RB...   \n",
            "4    (ROOT (S (NP (NNP Mr.) (NNP Brown)) (VP (VBD t...   \n",
            "5    (ROOT (UCP (PP (IN in) (NP (JJ cold) (NNS fram...   \n",
            "6    (ROOT (S (NP (PRP they)) (VP (VBD had) (NP (RB...   \n",
            "7    (ROOT (SINV (ADVP (RB long)) (VP (VBD haired))...   \n",
            "8    (ROOT (S (S (NP (PRP He)) (VP (VBZ 's) (VP (VB...   \n",
            "9    (ROOT (S (INTJ (UH Well)) (, ,) (ADVP (RB then...   \n",
            "10   (ROOT (S (S (NP (NNP Mother)) (VP (VBZ tells) ...   \n",
            "11   (ROOT (NP (NNP Renaissance) (NNP Florence) (. ...   \n",
            "12   (ROOT (S (NP (NNS Nebbishes)) (VP (VBP are) (A...   \n",
            "13   (ROOT (S (NP (PRP She)) (VP (VP (VBD was) (ADJ...   \n",
            "14   (ROOT (S (NP (NNP Operators)) (VP (VBP are) (V...   \n",
            "15   (ROOT (S (NP (PRP he)) (VP (VBD hit) (NP (PRP ...   \n",
            "16   (ROOT (S (NP (PRP We)) (VP (MD will) (VP (VB l...   \n",
            "17   (ROOT (S (NP (PRP I)) (VP (VBD saw) (NP (NP (C...   \n",
            "18   (ROOT (SINV (FRAG (INTJ (INTJ (WRB Why)) (, ,)...   \n",
            "19   (ROOT (S (CC Or) (, ,) (ADVP (RB rather)) (, ,...   \n",
            "20   (ROOT (S (NP (DT that) (JJ yeah) (FW i)) (VP (...   \n",
            "21   (ROOT (FRAG (ADVP (RB well)) (NP (NP (NNP Rick...   \n",
            "22   (ROOT (FRAG (CC and) (NP (NP (RB almost) (NN e...   \n",
            "23   (ROOT (FRAG (ADVP (DT all) (NN right)) (NP (NP...   \n",
            "24   (ROOT (S (NP (DT The) (NN audience)) (VP (VBD ...   \n",
            "25         (ROOT (NP (JJ individual) (NNS offenders)))   \n",
            "26   (ROOT (S (VP (VB Use) (NP (DT a) (JJ 360-degre...   \n",
            "27   (ROOT (S (VP (VB hum) (SBAR (IN so) (S (NP (PR...   \n",
            "28   (ROOT (S (NP (NNS Rooms)) (VP (VBP are) (ADJP ...   \n",
            "29   (ROOT (S (NP (DT The) (NN church)) (VP (VBZ da...   \n",
            "..                                                 ...   \n",
            "970  (ROOT (S (NP (PRP He)) (VP (VBD pointed) (PRT ...   \n",
            "971  (ROOT (S (NP (DT That)) (VP (VBZ 's) (NP (NP (...   \n",
            "972  (ROOT (S (NP (PRP It)) (VP (VBD was) (VP (ADVP...   \n",
            "973  (ROOT (S (NP (DT All) (NN right)) (VP (VBP say...   \n",
            "974  (ROOT (S (NP (NNP John)) (VP (VBZ maintains) (...   \n",
            "975  (ROOT (SINV (S (NP (PRP It)) (VP (VBZ 's) (ADJ...   \n",
            "976  (ROOT (S (NP (DT The) (NNP OTC)) (VP (VBZ cons...   \n",
            "977  (ROOT (NP (NNP Ranking) (NNP Minority) (NNP Me...   \n",
            "978  (ROOT (S (NP (DT This)) (VP (VBD was) (ADJP (J...   \n",
            "979  (ROOT (SINV (S (NP (PRP It)) (VP (VBZ is) (WHP...   \n",
            "980  (ROOT (S (NP (NNP Brill)) (VP (VBD grazed) (PP...   \n",
            "981  (ROOT (S (NP (NNS uh-huh)) (VP (VBZ sounds) (S...   \n",
            "982  (ROOT (S (NP (NN Brief)) (VP (VBZ intervention...   \n",
            "983  (ROOT (S (VP (VBG Implementing) (NP (NN screen...   \n",
            "984  (ROOT (S (VP (ADVP (RB no) (NP (DT no) (NN i))...   \n",
            "985  (ROOT (NP (NNP Dante) (NNP writes) (NNP Infern...   \n",
            "986  (ROOT (NP (NP (NNP Industrial) (NNP Economics)...   \n",
            "987  (ROOT (FRAG (NP (NN um-hum)) (ADJP (RB absolut...   \n",
            "988  (ROOT (NP (NNP Calling) (NNP Frank) (NNP Capra...   \n",
            "989  (ROOT (NP (NP (DT A) (JJ free) (NN ebook)) (PP...   \n",
            "990  (ROOT (SINV (VP (VBP yeah) (NP (PRP$ your) (NN...   \n",
            "991  (ROOT (S (NP (PRP I)) (VP (MD can) (ADVP (RB o...   \n",
            "992  (ROOT (SINV (ADVP (RB really)) (VP (VBP are) (...   \n",
            "993  (ROOT (S (NP (NNP Let) (POS 's)) (VP (VBP go) ...   \n",
            "994  (ROOT (SBAR (RB as) (IN if) (S (NP (PRP we)) (...   \n",
            "995  (ROOT (S (NP (DT The) (NN northerner)) (VP (VB...   \n",
            "996  (ROOT (PP (IN Over) (NP (NP (DT the) (NN entra...   \n",
            "997  (ROOT (S (NP (NNP Ponce) (FW de) (NNP Leen)) (...   \n",
            "998  (ROOT (S (NP (JJS Ca) (`` `) (NNS daan)) (VP (...   \n",
            "999  (ROOT (S (NP (JJ oh) (NN yeah) (NNS i)) (VP (V...   \n",
            "\n",
            "                                             sentence2  \\\n",
            "0    There are guardians that are more helpful than...   \n",
            "1                       She is not considering a move.   \n",
            "2               They probably changed it all the time.   \n",
            "3                          The rooms are all the same.   \n",
            "4                    Mr. Brown did not take his place.   \n",
            "5                 In cold frames or whatever they are.   \n",
            "6            They spent a lot of pleasant time there.    \n",
            "7                         Hair greater in length, yes.   \n",
            "8      Lincoln said the guy had to do it with friends.   \n",
            "9      Well then, let us return to where we came from.   \n",
            "10         I haven't spoken to anyone but you all day.   \n",
            "11                  The Renaissance is a time period.    \n",
            "12   Nebbishes are so pathetic, they are banned by ...   \n",
            "13        She was weak and her left side was immobile.   \n",
            "14                         Operators are here for you.   \n",
            "15            He hit her in the stomach with a hammer.   \n",
            "16               We will live like rats live in holes.   \n",
            "17      The officers were in good shape to be plucked.   \n",
            "18   The thought completely answered the question i...   \n",
            "19               \"There were two\", my companion noted.   \n",
            "20           I guess that takes effect rather quickly.   \n",
            "21                         It's been a pleasure, Rick.   \n",
            "22     I have cable, and so does nearly everyone else.   \n",
            "23     Ron we'll see you later at the party! Bye, bye.   \n",
            "24     The audience really wanted to see her do well.    \n",
            "25                                Multiple offenders.    \n",
            "26           Make use of a 360-degree feedback system.   \n",
            "27                          You're fishing downstream.   \n",
            "28                        Rooms are modern and chilly.   \n",
            "29           The church was built in the Gothic style.   \n",
            "..                                                 ...   \n",
            "970  He identified the criminals and the heist they...   \n",
            "971             That is Bradley's couch and footstool.   \n",
            "972                       It had been done very well.    \n",
            "973                     All right, do not say a thing.   \n",
            "974                        John is guilty in his eyes.   \n",
            "975                             Addams said it's easy    \n",
            "976                       The Governor is part of OTC.   \n",
            "977      Merited Small Member, A Noted Minority Person   \n",
            "978                             This was not the same.   \n",
            "979                                 Jon is giving up.    \n",
            "980      The Brill were hungry and had nothing to eat.   \n",
            "981   It sounds like you make dinners for your family.   \n",
            "982  They were worried they would drink themselves ...   \n",
            "983  Clinical practice is an area where screening m...   \n",
            "984                          No, I was born in Dallas.   \n",
            "985                                  He wrote Inferno.   \n",
            "986  Some people believe that industrial economics ...   \n",
            "987             Less than even might create a problem.   \n",
            "988             Nobody wants to deal with Frank Capra.   \n",
            "989                           The book is a paperback.   \n",
            "990     Yes, you are ruining it buy doing the opposite   \n",
            "991               The only thing I know is how I feel.   \n",
            "992                   So that's actually what you are.   \n",
            "993                              We'll never get them!   \n",
            "994                           It's like we were alone.   \n",
            "995         The northerner was breathing quite softly.   \n",
            "996                   A debate of entry into the USA.    \n",
            "997            Ponce de Leen had a number of setbacks.   \n",
            "998         Ca'daan realized that the man was smiling.   \n",
            "999                     I caught the end of the movie.   \n",
            "\n",
            "                                sentence2_binary_parse  \\\n",
            "0    ( There ( ( are ( guardians ( that ( are ( mor...   \n",
            "1    ( She ( ( ( is not ) ( considering ( a move ) ...   \n",
            "2    ( They ( probably ( ( ( changed it ) ( all ( t...   \n",
            "3    ( ( The rooms ) ( ( ( are all ) ( the same ) )...   \n",
            "4    ( ( Mr. Brown ) ( ( ( did not ) ( take ( his p...   \n",
            "5    ( ( In ( cold ( ( frames or ) whatever ) ) ) (...   \n",
            "6    ( They ( ( ( spent ( ( a lot ) ( of ( pleasant...   \n",
            "7    ( ( ( Hair greater ) ( in length ) ) ( , ( yes...   \n",
            "8    ( Lincoln ( ( said ( ( the guy ) ( had ( to ( ...   \n",
            "9    ( Well ( then ( , ( ( let ( us ( return ( to (...   \n",
            "10   ( I ( ( ( have n't ) ( spoken ( to ( anyone ( ...   \n",
            "11   ( ( The Renaissance ) ( ( is ( a ( time period...   \n",
            "12   ( ( Nebbishes ( are ( so pathetic ) ) ) ( , ( ...   \n",
            "13   ( ( ( ( She ( was weak ) ) and ) ( ( her ( lef...   \n",
            "14    ( Operators ( ( ( are here ) ( for you ) ) . ) )   \n",
            "15   ( He ( ( ( ( hit her ) ( in ( the stomach ) ) ...   \n",
            "16   ( We ( ( will ( live ( like ( rats ( live ( in...   \n",
            "17   ( ( The officers ) ( ( ( were ( in ( good shap...   \n",
            "18   ( ( The thought ) ( completely ( ( ( answered ...   \n",
            "19   ( ( `` ( There ( ( were two ) '' ) ) ) ( , ( (...   \n",
            "20   ( I ( ( guess ( that ( ( takes effect ) ( rath...   \n",
            "21   ( It ( ( 's ( been ( ( ( a pleasure ) , ) Rick...   \n",
            "22   ( I ( ( ( ( ( ( have cable ) , ) and ) so ) ( ...   \n",
            "23   ( Ron ( ( ( we ( 'll ( ( ( see you ) later ) (...   \n",
            "24   ( ( The audience ) ( really ( ( wanted ( to ( ...   \n",
            "25                        ( ( Multiple offenders ) . )   \n",
            "26   ( ( Make ( use ( of ( a ( 360-degree ( feedbac...   \n",
            "27        ( You ( ( 're ( fishing downstream ) ) . ) )   \n",
            "28   ( Rooms ( ( are ( ( modern and ) chilly ) ) . ) )   \n",
            "29   ( ( The church ) ( ( was ( built ( in ( the ( ...   \n",
            "..                                                 ...   \n",
            "970  ( ( He ( identified ( ( ( the criminals ) and ...   \n",
            "971  ( That ( ( is ( ( Bradley 's ) ( ( couch and )...   \n",
            "972  ( It ( ( had ( been ( done ( very well ) ) ) )...   \n",
            "973  ( ( All right ) ( , ( ( ( do not ) ( say ( a t...   \n",
            "974  ( John ( ( is ( guilty ( in ( his eyes ) ) ) )...   \n",
            "975             ( Addams ( said ( it ( 's easy ) ) ) )   \n",
            "976  ( ( The Governor ) ( ( is ( part ( of OTC ) ) ...   \n",
            "977  ( ( ( Merited ( Small Member ) ) , ) ( A ( Not...   \n",
            "978        ( This ( ( ( was not ) ( the same ) ) . ) )   \n",
            "979                 ( Jon ( ( is ( giving up ) ) . ) )   \n",
            "980  ( ( The Brill ) ( ( ( ( were hungry ) and ) ( ...   \n",
            "981  ( It ( ( sounds ( like ( you ( make ( dinners ...   \n",
            "982  ( They ( ( were ( worried ( they ( would ( ( d...   \n",
            "983  ( ( Clinical practice ) ( ( is ( ( an area ) (...   \n",
            "984  ( No ( , ( I ( ( was ( born ( in Dallas ) ) ) ...   \n",
            "985                     ( He ( ( wrote Inferno ) . ) )   \n",
            "986  ( ( Some people ) ( ( believe ( that ( ( indus...   \n",
            "987  ( Less ( ( than even ) ( ( might ( create ( a ...   \n",
            "988  ( Nobody ( ( wants ( to ( deal ( with ( Frank ...   \n",
            "989      ( ( The book ) ( ( is ( a paperback ) ) . ) )   \n",
            "990  ( Yes ( , ( you ( are ( ruining ( it ( buy ( d...   \n",
            "991  ( ( ( The ( only thing ) ) ( I know ) ) ( ( is...   \n",
            "992  ( So ( that ( ( ( 's actually ) ( what ( you a...   \n",
            "993        ( We ( ( ( 'll never ) ( get them ) ) ! ) )   \n",
            "994  ( It ( ( 's ( like ( we ( were alone ) ) ) ) ....   \n",
            "995  ( ( The northerner ) ( ( was ( ( breathing qui...   \n",
            "996  ( ( ( ( A debate ) ( of entry ) ) ( into ( the...   \n",
            "997  ( ( Ponce ( de Leen ) ) ( ( had ( ( a number )...   \n",
            "998  ( ( Ca ( ` daan ) ) ( ( realized ( that ( ( th...   \n",
            "999  ( I ( ( caught ( ( the end ) ( of ( the movie ...   \n",
            "\n",
            "                                       sentence2_parse  \n",
            "0    (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NP...  \n",
            "1    (ROOT (S (NP (PRP She)) (VP (VBZ is) (RB not) ...  \n",
            "2    (ROOT (S (NP (PRP They)) (ADVP (RB probably)) ...  \n",
            "3    (ROOT (S (NP (DT The) (NNS rooms)) (VP (VBP ar...  \n",
            "4    (ROOT (S (NP (NNP Mr.) (NNP Brown)) (VP (VBD d...  \n",
            "5    (ROOT (S (PP (IN In) (NP (JJ cold) (NNS frames...  \n",
            "6    (ROOT (S (NP (PRP They)) (VP (VBD spent) (NP (...  \n",
            "7    (ROOT (FRAG (PP (ADVP (NP (NNP Hair)) (JJR gre...  \n",
            "8    (ROOT (S (NP (NNP Lincoln)) (VP (VBD said) (SB...  \n",
            "9    (ROOT (S (ADVP (RB Well)) (ADVP (RB then)) (, ...  \n",
            "10   (ROOT (S (NP (PRP I)) (VP (VBP have) (RB n't) ...  \n",
            "11   (ROOT (S (NP (DT The) (NNP Renaissance)) (VP (...  \n",
            "12   (ROOT (S (S (NP (NNS Nebbishes)) (VP (VBP are)...  \n",
            "13   (ROOT (S (S (NP (PRP She)) (VP (VBD was) (ADJP...  \n",
            "14   (ROOT (S (NP (NNP Operators)) (VP (VBP are) (A...  \n",
            "15   (ROOT (S (NP (PRP He)) (VP (VBD hit) (NP (PRP ...  \n",
            "16   (ROOT (S (NP (PRP We)) (VP (MD will) (VP (VB l...  \n",
            "17   (ROOT (S (NP (DT The) (NNS officers)) (VP (VBD...  \n",
            "18   (ROOT (S (NP (DT The) (NN thought)) (ADVP (RB ...  \n",
            "19   (ROOT (S (S (`` ``) (NP (EX There)) (VP (VBD w...  \n",
            "20   (ROOT (S (NP (PRP I)) (VP (VBP guess) (SBAR (I...  \n",
            "21   (ROOT (S (NP (PRP It)) (VP (VBZ 's) (VP (VBN b...  \n",
            "22   (ROOT (S (NP (PRP I)) (VP (VP (VBP have) (NP (...  \n",
            "23   (ROOT (S (NP (NNP Ron)) (PRN (S (NP (PRP we)) ...  \n",
            "24   (ROOT (S (NP (DT The) (NN audience)) (ADVP (RB...  \n",
            "25   (ROOT (S (VP (VB Multiple) (NP (NNS offenders)...  \n",
            "26   (ROOT (S (VP (VB Make) (NP (NP (NN use)) (PP (...  \n",
            "27   (ROOT (S (NP (PRP You)) (VP (VBP 're) (PP (NN ...  \n",
            "28   (ROOT (S (NP (NNS Rooms)) (VP (VBP are) (ADJP ...  \n",
            "29   (ROOT (S (NP (DT The) (NN church)) (VP (VBD wa...  \n",
            "..                                                 ...  \n",
            "970  (ROOT (S (S (NP (PRP He)) (VP (VBD identified)...  \n",
            "971  (ROOT (S (NP (DT That)) (VP (VBZ is) (NP (NP (...  \n",
            "972  (ROOT (S (NP (PRP It)) (VP (VBD had) (VP (VBN ...  \n",
            "973  (ROOT (S (NP (DT All) (NN right)) (, ,) (VP (V...  \n",
            "974  (ROOT (S (NP (NNP John)) (VP (VBZ is) (ADJP (J...  \n",
            "975  (ROOT (S (NP (NNS Addams)) (VP (VBD said) (SBA...  \n",
            "976  (ROOT (S (NP (DT The) (NNP Governor)) (VP (VBZ...  \n",
            "977  (ROOT (NP (NP (NNP Merited) (NNP Small) (NNP M...  \n",
            "978  (ROOT (S (NP (DT This)) (VP (VBD was) (RB not)...  \n",
            "979  (ROOT (S (NP (NNP Jon)) (VP (VBZ is) (VP (VBG ...  \n",
            "980  (ROOT (S (NP (DT The) (NNP Brill)) (VP (VP (VB...  \n",
            "981  (ROOT (S (NP (PRP It)) (VP (VBZ sounds) (SBAR ...  \n",
            "982  (ROOT (S (NP (PRP They)) (VP (VBD were) (VP (V...  \n",
            "983  (ROOT (S (NP (JJ Clinical) (NN practice)) (VP ...  \n",
            "984  (ROOT (S (INTJ (UH No)) (, ,) (NP (PRP I)) (VP...  \n",
            "985  (ROOT (S (NP (PRP He)) (VP (VBD wrote) (NP (NN...  \n",
            "986  (ROOT (S (NP (DT Some) (NNS people)) (VP (VBP ...  \n",
            "987  (ROOT (S (NP (NNP Less)) (ADVP (IN than) (RB e...  \n",
            "988  (ROOT (S (NP (NN Nobody)) (VP (VBZ wants) (S (...  \n",
            "989  (ROOT (S (NP (DT The) (NN book)) (VP (VBZ is) ...  \n",
            "990  (ROOT (S (ADVP (RB Yes)) (, ,) (NP (PRP you)) ...  \n",
            "991  (ROOT (S (NP (NP (DT The) (JJ only) (NN thing)...  \n",
            "992  (ROOT (S (IN So) (NP (DT that)) (VP (VBZ 's) (...  \n",
            "993  (ROOT (S (NP (PRP We)) (VP (MD 'll) (ADVP (RB ...  \n",
            "994  (ROOT (S (NP (PRP It)) (VP (VBZ 's) (SBAR (IN ...  \n",
            "995  (ROOT (S (NP (DT The) (NN northerner)) (VP (VB...  \n",
            "996  (ROOT (NP (NP (DT A) (NN debate)) (PP (IN of) ...  \n",
            "997  (ROOT (S (NP (NNP Ponce) (FW de) (NNP Leen)) (...  \n",
            "998  (ROOT (S (NP (JJS Ca) (`` `) (NNS daan)) (VP (...  \n",
            "999  (ROOT (S (NP (PRP I)) (VP (VBD caught) (NP (NP...  \n",
            "\n",
            "[1000 rows x 11 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVh8QAdgVU5C",
        "colab_type": "code",
        "outputId": "c743f6b9-ef1c-4316-f820-516460305f2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "#Downloading GLOVE\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-08 18:32:51--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2019-04-08 18:32:52--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  17.0MB/s    in 64s     \n",
            "\n",
            "2019-04-08 18:33:56 (12.9 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYsLJBVDfDg1",
        "colab_type": "code",
        "outputId": "a6895ba3-4c12-4739-92a2-0137a809f626",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXnZxp0Ae9nC",
        "colab_type": "code",
        "outputId": "358402e5-9352-47b7-a982-42298a2a7b7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Creating word graph \n",
        "word2vec = {}\n",
        "with open('glove.6B.50d.txt', encoding=\"utf8\") as f:\n",
        "  # is just a space-separated text file in the format:\n",
        "  # word vec[0] vec[1] vec[2] ...\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vec = np.asarray(values[1:], dtype='float32')\n",
        "        word2vec[word] = vec\n",
        "print('Found %s word vectors.' % len(word2vec))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_Ugejv4WSBv",
        "colab_type": "code",
        "outputId": "3966f313-3c8f-43a9-957c-4c09ad2192c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#removing '-' from test labels and replacing with 'neutral'\n",
        "print(test.shape)\n",
        "test = test[test.gold_label != '-']\n",
        "print(test.shape)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 11)\n",
            "(990, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swstrJ1Q77lR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_embeddings(dataset):\n",
        "  #create embedding matrix from premise and hypothesis\n",
        " \n",
        "  \n",
        "  premise_embedding_matrix =np.empty((750,), float)\n",
        "  hypothesis_embedding_matrix =np.empty((750,), float)\n",
        "  premise_sentence_length = []\n",
        "  hypothesis_sentence_length = []\n",
        "  embedding_vec_np=[]\n",
        "  \n",
        "  \n",
        "  #tokenize sentences\n",
        "  for row in dataset['sentence1']:\n",
        "    flatten = []\n",
        "    word_sent = row.split()\n",
        "    tokenizer = Tokenizer( filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True, split=' ')\n",
        "    tokenizer.fit_on_texts(word_sent)\n",
        "    word_sent_dict = tokenizer.word_index\n",
        "    premise_sentence_length = np.append(premise_sentence_length,len(word_sent_dict))\n",
        "    #print(neg)\n",
        "    #remove words that do not have word vector in glove\n",
        "    t=0\n",
        "    for word,i in word_sent_dict.items():\n",
        "      #get the vectors for each words\n",
        "      embedding_vector = word2vec.get(word)\n",
        "   \n",
        "\n",
        "      if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all zeros.\n",
        "        flatten= np.asarray(flatten)\n",
        "        #print(premise_1.shape)\n",
        "        flatten = np.append(flatten,embedding_vector)\n",
        "        t= t+1\n",
        "        \n",
        "        \n",
        "    #print(flatten.shape) \n",
        "    zero_pad = 15 - t\n",
        "    arr = [0]*50*zero_pad\n",
        "    flatten = np.append(flatten, arr)\n",
        "    #print(flatten.shape) \n",
        "    premise_embedding_matrix = np.vstack((premise_embedding_matrix,flatten))\n",
        "  premise_embedding_matrix = np.delete(premise_embedding_matrix, 0, 0)\n",
        "          \n",
        "  #print(premise_embedding_matrix.shape)\n",
        "  #print(len(premise_sentence_length))\n",
        "\n",
        "  \n",
        "  for row in dataset['sentence2']:\n",
        "    flatten = []\n",
        "    word_sent = row.split()\n",
        "    tokenizer = Tokenizer( filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True, split=' ')\n",
        "    tokenizer.fit_on_texts(word_sent)\n",
        "    word_sent_dict = tokenizer.word_index\n",
        "    hypothesis_sentence_length = np.append(hypothesis_sentence_length,len(word_sent_dict))\n",
        "    #print(neg)\n",
        "    #remove words that do not have word vector in glove\n",
        "    t=0\n",
        "    for word,i in word_sent_dict.items():\n",
        "      \n",
        "      embedding_vector = word2vec.get(word)\n",
        "\n",
        "      if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all zeros.\n",
        "        flatten= np.asarray(flatten)\n",
        "        #print(premise_1.shape)\n",
        "        flatten = np.append(flatten,embedding_vector)\n",
        "        t= t+1\n",
        "        \n",
        "        \n",
        "    #print(flatten.shape) \n",
        "    zero_pad = 15 - t\n",
        "    arr = [0]*50*zero_pad\n",
        "    flatten = np.append(flatten, arr)\n",
        "    #print(flatten.shape) \n",
        "    hypothesis_embedding_matrix = np.vstack((hypothesis_embedding_matrix,flatten))\n",
        "  hypothesis_embedding_matrix = np.delete(hypothesis_embedding_matrix, 0, 0)\n",
        "  #convert labels to integers\n",
        "  target = np.array(dataset['gold_label'])\n",
        "  #target.reshape(1000,1)\n",
        "  for index,i in enumerate(target):\n",
        "    if i == 'neutral':\n",
        "      target[index] = 0\n",
        "    elif i == 'entailment':\n",
        "      target[index] = 1\n",
        "    elif i == 'contradiction':\n",
        "      target[index] = 2\n",
        "\n",
        "  length = len(target)\n",
        "  one_hot_target = np.empty((3,), int)\n",
        "  for index, i in enumerate(target):\n",
        "    if i == 0:\n",
        "      c = [1,0,0]\n",
        "    elif i == 1:\n",
        "      c = [0,1,0]\n",
        "    elif i == 2:\n",
        "      c = [0,0,1]\n",
        "    one_hot_target = np.vstack([one_hot_target,c])\n",
        "      \n",
        "  \n",
        "        \n",
        "  print('shape of premise embeddings',premise_embedding_matrix.shape)\n",
        "  print('shape of hypothesis embeddings',hypothesis_embedding_matrix.shape)\n",
        "  return premise_embedding_matrix, hypothesis_embedding_matrix, one_hot_target\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2LQKum-SRrB",
        "colab_type": "code",
        "outputId": "ac7761e2-4d5a-4728-c93a-75a284bf93ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "print('dev')\n",
        "premise_embedding_matrix_dev, hypothesis_embedding_matrix_dev, target_dev  = word_embeddings(dev) \n",
        "print('train')\n",
        "premise_embedding_matrix_train, hypothesis_embedding_matrix_train, target_train = word_embeddings(train) \n",
        "print('test')\n",
        "premise_embedding_matrix_test, hypothesis_embedding_matrix_test, target_test  = word_embeddings(test) \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dev\n",
            "shape of premise embeddings (1000, 750)\n",
            "shape of hypothesis embeddings (1000, 750)\n",
            "train\n",
            "shape of premise embeddings (10000, 750)\n",
            "shape of hypothesis embeddings (10000, 750)\n",
            "test\n",
            "shape of premise embeddings (990, 750)\n",
            "shape of hypothesis embeddings (990, 750)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OseuEb2NXt98",
        "colab_type": "code",
        "outputId": "24c1c3b3-a3d7-473c-ff54-9f357177c5ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "def remove_first(arr):\n",
        "  arr = np.delete(arr, 0, 0)\n",
        "  return arr\n",
        "  \n",
        "  \n",
        "target_dev = remove_first(target_dev)\n",
        "target_train = remove_first(target_train)\n",
        "target_test = remove_first(target_test)\n",
        "print(target_dev.shape)\n",
        "print(target_train.shape)\n",
        "print(target_test.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 3)\n",
            "(10000, 3)\n",
            "[[1 0 0]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " ...\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [0 1 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKqsugMKQUYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#normalizing the datasets\n",
        "scaler = StandardScaler()\n",
        "premise_embedding_matrix_dev = scaler.fit_transform(premise_embedding_matrix_dev)\n",
        "hypothesis_embedding_matrix_dev = scaler.fit_transform(hypothesis_embedding_matrix_dev)\n",
        "premise_embedding_matrix_train = scaler.fit_transform(premise_embedding_matrix_train)\n",
        "hypothesis_embedding_matrix_train = scaler.fit_transform(hypothesis_embedding_matrix_train)\n",
        "premise_embedding_matrix_test = scaler.fit_transform(premise_embedding_matrix_test)\n",
        "hypothesis_embedding_matrix_test = scaler.fit_transform(hypothesis_embedding_matrix_test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmtBdW6UN1nC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model 1\n",
        "pre_inputs = Input(shape=(750,))\n",
        "x = Dense(750, activation='relu')(pre_inputs)\n",
        "x = Dense(500, activation='relu')(x)\n",
        "x = Dense(100, activation='sigmoid')(x)\n",
        "\n",
        "premise_output = x\n",
        "\n",
        "hyp_inputs = Input(shape=(750,))\n",
        "y = Dense(750, activation='relu')(hyp_inputs)\n",
        "y = Dense(500, activation='relu')(y)\n",
        "y = Dense(100, activation='sigmoid')(y)\n",
        "\n",
        "hypothesis_output = y\n",
        "\n",
        "g = keras.layers.concatenate([premise_output, hypothesis_output])\n",
        "\n",
        "g = Dense(200, activation='relu')(g)\n",
        "g = Dense(100, activation='relu')(g)\n",
        "main_output = Dense(3, activation='relu', name='main_output')(g)\n",
        "\n",
        "model = Model(inputs=[pre_inputs,hyp_inputs], outputs=[main_output])\n",
        "opt = optimizers.SGD(lr=0.20, decay=1e-2, momentum=0.9, nesterov=True)\n",
        "model.compile(optimizer=opt, loss='mean_squared_error', loss_weights=[0.6], metrics=['accuracy'])\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0jv0F1ZRLQF",
        "colab_type": "code",
        "outputId": "58454405-ce13-47ad-9a74-ee6a26287222",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798
        }
      },
      "source": [
        "model.fit([premise_embedding_matrix_dev, hypothesis_embedding_matrix_dev], target_dev,epochs=10, batch_size=30)\n",
        "model.fit([premise_embedding_matrix_train, hypothesis_embedding_matrix_train], target_train,epochs=10, batch_size=30)\n",
        "model.evaluate([premise_embedding_matrix_test, hypothesis_embedding_matrix_test], target_test, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/10\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2366 - acc: 0.2930\n",
            "Epoch 2/10\n",
            "1000/1000 [==============================] - 1s 731us/step - loss: 0.2000 - acc: 0.2800\n",
            "Epoch 3/10\n",
            "1000/1000 [==============================] - 1s 741us/step - loss: 0.2000 - acc: 0.2800\n",
            "Epoch 4/10\n",
            "1000/1000 [==============================] - 1s 743us/step - loss: 0.2000 - acc: 0.2800\n",
            "Epoch 5/10\n",
            "1000/1000 [==============================] - 1s 739us/step - loss: 0.2000 - acc: 0.2800\n",
            "Epoch 6/10\n",
            "1000/1000 [==============================] - 1s 732us/step - loss: 0.2000 - acc: 0.2800\n",
            "Epoch 7/10\n",
            "1000/1000 [==============================] - 1s 733us/step - loss: 0.2000 - acc: 0.2800\n",
            "Epoch 8/10\n",
            "1000/1000 [==============================] - 1s 732us/step - loss: 0.2000 - acc: 0.2800\n",
            "Epoch 9/10\n",
            "1000/1000 [==============================] - 1s 748us/step - loss: 0.2000 - acc: 0.2800\n",
            "Epoch 10/10\n",
            "1000/1000 [==============================] - 1s 727us/step - loss: 0.2000 - acc: 0.2800\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 7s 702us/step - loss: 0.2000 - acc: 0.2640\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 7s 709us/step - loss: 0.2000 - acc: 0.2640\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 7s 710us/step - loss: 0.2000 - acc: 0.2640\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 7s 713us/step - loss: 0.2000 - acc: 0.2640\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 7s 713us/step - loss: 0.2000 - acc: 0.2640\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 7s 718us/step - loss: 0.2000 - acc: 0.2640\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 7s 708us/step - loss: 0.2000 - acc: 0.2640\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 7s 704us/step - loss: 0.2000 - acc: 0.2640\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 7s 708us/step - loss: 0.2000 - acc: 0.2640\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 7s 704us/step - loss: 0.2000 - acc: 0.2640\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.20000001788139343, 0.27676767712891703]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0HmUy5lbLAS-",
        "colab": {}
      },
      "source": [
        "#model 2\n",
        "pre_inputs = Input(shape=(750,))\n",
        "x = Dense(750, activation='sigmoid')(pre_inputs)\n",
        "x = Dense(500, activation='sigmoid')(x)\n",
        "x = Dense(100, activation='sigmoid')(x)\n",
        "\n",
        "premise_output = x\n",
        "\n",
        "hyp_inputs = Input(shape=(750,))\n",
        "y = Dense(750, activation='sigmoid')(hyp_inputs)\n",
        "y = Dense(500, activation='sigmoid')(y)\n",
        "y = Dense(100, activation='sigmoid')(y)\n",
        "\n",
        "hypothesis_output = y\n",
        "\n",
        "g = keras.layers.concatenate([premise_output, hypothesis_output])\n",
        "\n",
        "g = Dense(200, activation='sigmoid')(g)\n",
        "g = Dense(50, activation='sigmoid')(g)\n",
        "main_output = Dense(3, activation='relu', name='main_output')(g)\n",
        "\n",
        "model2 = Model(inputs=[pre_inputs,hyp_inputs], outputs=[main_output])\n",
        "opt = optimizers.SGD(lr=0.20, decay=1e-2, momentum=0.9, nesterov=True)\n",
        "model2.compile(optimizer=opt, loss='mean_squared_error', loss_weights=[0.6], metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuyBcGwsOEI0",
        "colab_type": "code",
        "outputId": "b044ce92-7107-45eb-9635-e982b012c9fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        }
      },
      "source": [
        "model2.fit([premise_embedding_matrix_dev, hypothesis_embedding_matrix_dev], target_dev,epochs=10, batch_size=30)\n",
        "model2.fit([premise_embedding_matrix_train, hypothesis_embedding_matrix_train], target_train,epochs=10, batch_size=30)\n",
        "model2.evaluate([premise_embedding_matrix_test, hypothesis_embedding_matrix_test], target_test, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1913 - acc: 0.3800\n",
            "Epoch 2/10\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1913 - acc: 0.3800\n",
            "Epoch 3/10\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1913 - acc: 0.3800\n",
            "Epoch 4/10\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1913 - acc: 0.3800\n",
            "Epoch 5/10\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1913 - acc: 0.3800\n",
            "Epoch 6/10\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1913 - acc: 0.3800\n",
            "Epoch 7/10\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1913 - acc: 0.3800\n",
            "Epoch 8/10\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1913 - acc: 0.3800\n",
            "Epoch 9/10\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1913 - acc: 0.3800\n",
            "Epoch 10/10\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1913 - acc: 0.3800\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 14s 1ms/step - loss: 0.3750 - acc: 0.3544\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 14s 1ms/step - loss: 0.3750 - acc: 0.3544\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 14s 1ms/step - loss: 0.3750 - acc: 0.3544\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 14s 1ms/step - loss: 0.3750 - acc: 0.3544\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 14s 1ms/step - loss: 0.3750 - acc: 0.3544\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 14s 1ms/step - loss: 0.3750 - acc: 0.3544\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 14s 1ms/step - loss: 0.3750 - acc: 0.3544\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 14s 1ms/step - loss: 0.3750 - acc: 0.3544\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 14s 1ms/step - loss: 0.3750 - acc: 0.3544\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 14s 1ms/step - loss: 0.3750 - acc: 0.3544\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.1944971103668213, 0.371]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gqd4nIaKhJXi",
        "colab": {}
      },
      "source": [
        "#model 3\n",
        "pre_inputs = Input(shape=(750,))\n",
        "x = Dense(750, activation='sigmoid')(pre_inputs)\n",
        "x = Dense(500, activation='sigmoid')(x)\n",
        "x = Dense(300, activation='sigmoid')(x)\n",
        "\n",
        "premise_output = x\n",
        "\n",
        "hyp_inputs = Input(shape=(750,))\n",
        "y = Dense(750, activation='sigmoid')(hyp_inputs)\n",
        "y = Dense(500, activation='sigmoid')(y)\n",
        "y = Dense(300, activation='sigmoid')(y)\n",
        "\n",
        "hypothesis_output = y\n",
        "\n",
        "g = keras.layers.concatenate([premise_output, hypothesis_output])\n",
        "\n",
        "g = Dense(200, activation='sigmoid')(g)\n",
        "g = Dense(100, activation='sigmoid')(g)\n",
        "main_output = Dense(3, activation='relu', name='main_output')(g)\n",
        "\n",
        "model3 = Model(inputs=[pre_inputs,hyp_inputs], outputs=[main_output])\n",
        "opt = optimizers.SGD(lr=0.20, decay=1e-2, momentum=0.9, nesterov=True)\n",
        "model3.compile(optimizer=opt, loss='mean_squared_error', loss_weights=[0.6], metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNiES_Q7hWkJ",
        "colab_type": "code",
        "outputId": "42f15ca8-70a9-40be-c4b4-f7c5a7043a26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        }
      },
      "source": [
        "model3.fit([premise_embedding_matrix_dev, hypothesis_embedding_matrix_dev], target_dev,epochs=10, batch_size=30)\n",
        "model3.fit([premise_embedding_matrix_train, hypothesis_embedding_matrix_train], target_train,epochs=10, batch_size=30)\n",
        "model3.evaluate([premise_embedding_matrix_test, hypothesis_embedding_matrix_test], target_test, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1000/1000 [==============================] - 3s 3ms/step - loss: 1.5220 - acc: 0.3320\n",
            "Epoch 2/10\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 0.9000 - acc: 0.3400\n",
            "Epoch 3/10\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 0.9000 - acc: 0.3400\n",
            "Epoch 4/10\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 0.9000 - acc: 0.3400\n",
            "Epoch 5/10\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 0.9000 - acc: 0.3400\n",
            "Epoch 6/10\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 0.9000 - acc: 0.3400\n",
            "Epoch 7/10\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 0.9000 - acc: 0.3400\n",
            "Epoch 8/10\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 0.9000 - acc: 0.3400\n",
            "Epoch 9/10\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 0.9000 - acc: 0.3400\n",
            "Epoch 10/10\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 0.9000 - acc: 0.3400\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 12s 1ms/step - loss: 0.8462 - acc: 0.3816\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 12s 1ms/step - loss: 0.8462 - acc: 0.3816\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 11s 1ms/step - loss: 0.8462 - acc: 0.3816\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 12s 1ms/step - loss: 0.8462 - acc: 0.3816\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 11s 1ms/step - loss: 0.8462 - acc: 0.3816\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 12s 1ms/step - loss: 0.8462 - acc: 0.3816\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 12s 1ms/step - loss: 0.8462 - acc: 0.3816\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 12s 1ms/step - loss: 0.8462 - acc: 0.3816\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 12s 1ms/step - loss: 0.8462 - acc: 0.3816\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 11s 1ms/step - loss: 0.8462 - acc: 0.3816\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9042000379562378, 0.345]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LikgWsCfh6dG",
        "colab": {}
      },
      "source": [
        "#model 4\n",
        "pre_inputs = Input(shape=(750,))\n",
        "x = Dense(750, activation='sigmoid')(pre_inputs)\n",
        "x = Dense(500, activation='sigmoid')(x)\n",
        "x = Dense(300, activation='sigmoid')(x)\n",
        "\n",
        "premise_output = x\n",
        "\n",
        "hyp_inputs = Input(shape=(750,))\n",
        "y = Dense(750, activation='sigmoid')(hyp_inputs)\n",
        "y = Dense(500, activation='sigmoid')(y)\n",
        "y = Dense(300, activation='sigmoid')(y)\n",
        "\n",
        "hypothesis_output = y\n",
        "\n",
        "g = keras.layers.concatenate([premise_output, hypothesis_output])\n",
        "\n",
        "g = Dense(200, activation='sigmoid')(g)\n",
        "g = Dense(50, activation='sigmoid')(g)\n",
        "main_output = Dense(3, activation='relu', name='main_output')(g)\n",
        "\n",
        "model4 = Model(inputs=[pre_inputs,hyp_inputs], outputs=[main_output])\n",
        "opt = optimizers.SGD(lr=0.20, decay=1e-2, momentum=0.9, nesterov=True)\n",
        "model4.compile(optimizer=opt, loss='mean_squared_error', loss_weights=[0.6], metrics=['accuracy'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1be4afcb-4bae-461d-f5bf-3e1859fa6133",
        "id": "32TwtF1Yh6dM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        }
      },
      "source": [
        "model4.fit([premise_embedding_matrix_dev, hypothesis_embedding_matrix_dev], target_dev,epochs=10, batch_size=30)\n",
        "model4.fit([premise_embedding_matrix_train, hypothesis_embedding_matrix_train], target_train,epochs=10, batch_size=30)\n",
        "model4.evaluate([premise_embedding_matrix_test, hypothesis_embedding_matrix_test], target_test, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 0.9000 - acc: 0.3400\n",
            "Epoch 2/10\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 0.9000 - acc: 0.3400\n",
            "Epoch 3/10\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 0.9000 - acc: 0.3400\n",
            "Epoch 4/10\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 0.9000 - acc: 0.3400\n",
            "Epoch 5/10\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 0.9000 - acc: 0.3400\n",
            "Epoch 6/10\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 0.9000 - acc: 0.3400\n",
            "Epoch 7/10\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 0.9000 - acc: 0.3400\n",
            "Epoch 8/10\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 0.9000 - acc: 0.3400\n",
            "Epoch 9/10\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 0.9000 - acc: 0.3400\n",
            "Epoch 10/10\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 0.9000 - acc: 0.3400\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 10s 1ms/step - loss: 0.8462 - acc: 0.3816\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 10s 1ms/step - loss: 0.8462 - acc: 0.3816\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 10s 1ms/step - loss: 0.8462 - acc: 0.3816\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 10s 1ms/step - loss: 0.8462 - acc: 0.3816\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 10s 1ms/step - loss: 0.8462 - acc: 0.3816\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 11s 1ms/step - loss: 0.8462 - acc: 0.3816\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 10s 1ms/step - loss: 0.8462 - acc: 0.3816\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 10s 1ms/step - loss: 0.8462 - acc: 0.3816\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 10s 1ms/step - loss: 0.8462 - acc: 0.3816\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 10s 1ms/step - loss: 0.8462 - acc: 0.3816\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9042000379562378, 0.345]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    }
  ]
}